{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Research_Problem_and_Code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParvinGhaffarzadeh/B-K-G-for-NLP-Research/blob/main/Research_Problem_and_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re8T7-D7J2Wd"
      },
      "source": [
        "# **Code.txt** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qare3uLHK89l"
      },
      "source": [
        "#### Load the required packages\n",
        "import os\n",
        "import re"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Uk1xLLx18Qwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEYc1tZlKPA2"
      },
      "source": [
        "#### Point it to the directory where your Test Data is present\n",
        "input_dir = \"/content/drive/MyDrive/sub3_ph22/\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBLLi78yKwqc"
      },
      "source": [
        "#### List all the folders present in the directory\n",
        "list_of_folders = [\"constituency_parsing\",\"coreference_resolution\",\n",
        "                   \"data-to-text_generation\",\"dependency_parsing\",\n",
        "                   \"document_classification\",\"entity_linking\",\n",
        "                   \"face_alignment\",\"face_detection\", \"hypernym_discovery\",\n",
        "                   \"natural_language_inference\"]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF7weUGLKPA8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "c35d1860-6f32-4ade-f0c5-68d4989a0581"
      },
      "source": [
        "#### Load the Test Data\n",
        "input_stanza_list = [] #Stores individual lines from Stanza_out.txt file \n",
        "input_stanza_len = [] #Stores the number of lines present in Stanza_out.txt file \n",
        "input_sent_num_list = [] #Stores individual lines from sentences.txt file\n",
        "file_name_list = [] #Stores the name of individual files\n",
        "\n",
        "for fls in list_of_folders:\n",
        "  count=0\n",
        "  for i in os.listdir(input_dir + fls + '/'):\n",
        "    for files in os.listdir(input_dir + fls + '/' + i + \"/\"):\n",
        "      if files.endswith(\"Stanza-out.txt\"):\n",
        "        stanza_file = open(input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
        "        stanza_lines = stanza_file.read()\n",
        "        stanza_lines_list = list(filter(None,map(lambda x:x,stanza_lines.splitlines()))) # filter empty strings and split into lines\n",
        "        input_stanza_len.append(len(stanza_lines_list))\n",
        "        input_stanza_list.append(stanza_lines_list)\n",
        "      if files.endswith(\"sentences.txt\"):\n",
        "        sentence_file = open(input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
        "        sentence_num_list = list(filter(None,sentence_file.read().splitlines())) # filter empty strings and split into lines\n",
        "        input_sent_num_list.append(list(map(int, sentence_num_list)))\n",
        "    count=count+1\n",
        "         \n",
        "    file_name_list.append(fls + '/' + str(i))\n",
        "  print(\"completed\",fls, count)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-c7fc615f4a86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_of_folders\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfls\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfls\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Stanza-out.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/sub3_ph22/constituency_parsing/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ch58RJM5LvAL"
      },
      "source": [
        "print(len(input_sent_num_list))\n",
        "print()\n",
        "print(input_sent_num_list[1])\n",
        "print()\n",
        "print(len(file_name_list))\n",
        "print()\n",
        "print(file_name_list[1])\n",
        "print()\n",
        "print(len(input_stanza_list))\n",
        "print()\n",
        "print(input_stanza_list[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHnvGotWNY9Y"
      },
      "source": [
        "#### Returns a list of urls present in a string using a regex expression. In case no url is present, it returns an empty list. \n",
        "def find_url(string): \n",
        "    regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
        "    url = re.findall(regex,string)       \n",
        "    return [x[0] for x in url] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mImoePk0M3S-"
      },
      "source": [
        "#### The main function to identify where the Code information unit should be present and what should be its triples\n",
        "\n",
        "for i in zip(input_sent_num_list, input_stanza_list, file_name_list):\n",
        "  for sen_num in i[0]:\n",
        "\n",
        "    # Get the contribution sentence corresponding to its sentence number in sentences.txt file\n",
        "    j = i[1][(int)(sen_num)-1]  \n",
        "\n",
        "    # Now we remove all the useless spaces which can be present in our url (eg. \"https:// wikipedia .com\" converted to \"https://wikipedia.com\")   \n",
        "    # temp1 removes all leading and trailing spaces around \"/\"\n",
        "    temp1 = re.sub(r'(?:(?<=\\/) | (?=\\/))','',j)\n",
        "    # temp2 removes all leading and trailing spaces around \":\"\n",
        "    temp2 = re.sub(r'(?:(?<=\\:) | (?=\\:))','',temp1)\n",
        "    # temp3 removes all leading and trailing spaces around \".\"\n",
        "    temp3 = re.sub(r'(?:(?<=\\.) | (?=\\.))','',temp2)\n",
        "    # temp4 removes all leading and trailing spaces around \"-\"\n",
        "    temp4 = re.sub(r'(?:(?<=\\-) | (?=\\-))','',temp3)\n",
        "    # Now temp4 is used as the final sentence in which urls have to be found\n",
        "\n",
        "    # list_urls contains the list of urls found in that sentence\n",
        "    list_urls = find_url(temp4)\n",
        "\n",
        "    # If some url is found in that sentence, then proceed forward\n",
        "    if (len(list_urls)!=0): \n",
        "      # Only those urls considered in Code Information unit whose corresponding sentence consists of words like \"our\", \"code\", \"codes\"\n",
        "      if (\"our\" in j.lower().split(\" \")) or (\"code\" in j.lower().split(\" \"))or (\"codes\" in j.lower().split(\" \")):\n",
        "\n",
        "        # Now we identify the starting and ending character number of the url in its respective sentence\n",
        "        user_iterator = re.finditer('http', j.lower())\n",
        "\n",
        "        # This list consists of the starting character number of the urls\n",
        "        user_list = [user.start() for user in user_iterator]  \n",
        "        \n",
        "        for k in zip(user_list, list_urls):\n",
        "          start = k[0]\n",
        "          end = start \n",
        "          length_of_link = len(k[1])\n",
        "          while (end<len(j))and(length_of_link!=0):\n",
        "            # we ignore all the spaces occuring in between the url\n",
        "            if (j[end]!=\" \"):\n",
        "              length_of_link = length_of_link - 1\n",
        "            # \"end\" stores the ending character number of the urls\n",
        "            end = end + 1\n",
        "\n",
        "          # Check the url in the original sentence using start and end character number   \n",
        "          print(j[start:end])\n",
        "          # Print the respective file name also\n",
        "          print(i[2])\n",
        "\n",
        "          # Write the extracted url along with its starting and ending character number, in the entities.txt file \n",
        "          output_file1 = open(input_dir + i[2] + \"/entities.txt\",\"a\")\n",
        "          output_file1.write(str(sen_num) + \"\\t\" + str(start) + \"\\t\" + str(end) + \"\\t\" + j[start:end] + \"\\n\")\n",
        "          output_file1.close() \n",
        "          # Make and write the extracted url in the Code.txt file \n",
        "          output_file2 = open(input_dir + i[2] + \"/triples/\" + \"code.txt\",\"a\")\n",
        "          output_file2.write(\"(Contribution||Code||\" + j[start:end] + \")\" + \"\\n\")\n",
        "          output_file2.close()  \n",
        "\n",
        "        print()      \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lx8lIx7htEu"
      },
      "source": [
        "# **Research_problem.txt**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkzEBxJ8iDYj"
      },
      "source": [
        "#### Load the required packages\n",
        "import os\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUFlqfVTh7cv"
      },
      "source": [
        "#### Point it to the directory where your Test Data is present\n",
        "input_dir = \"/content/drive/MyDrive/sub3_ph22/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw8gTcBSiGdP"
      },
      "source": [
        "#### List all the folders present in the directory\n",
        "list_of_folders = [\"constituency_parsing\",\"coreference_resolution\",\n",
        "                   \"data-to-text_generation\",\"dependency_parsing\",\n",
        "                   \"document_classification\",\"entity_linking\",\n",
        "                   \"face_alignment\",\"face_detection\", \"hypernym_discovery\",\n",
        "                   \"natural_language_inference\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNjQL5BKh2gy"
      },
      "source": [
        "#### Load Test Data\n",
        "file_name_list = [] #Stores the name of all files \n",
        "input_entity_list = [] #Stores the list of all phrases from entities.txt\n",
        "\n",
        "for fls in list_of_folders:\n",
        "  count=0\n",
        "  for i in os.listdir(input_dir + fls + '/'):\n",
        "    for files in os.listdir(input_dir + fls + '/' + i + \"/\"):\n",
        "      if files.endswith(\"entities.txt\"):\n",
        "        entities_file = open(input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
        "        entities_list = list(filter(None,(entities_file.read()).splitlines())) # filter empty strings and split into lines\n",
        "        input_entity_list.append(entities_list)  \n",
        "    count=count+1\n",
        "    \n",
        "    file_name_list.append(fls + '/' + str(i))\n",
        "    \n",
        "  print(\"completed\",fls, count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v1wE3QhCH8y"
      },
      "source": [
        "#### Returns a list of urls present in a string using a regex expression. In case no url is present, it returns an empty list. \n",
        "def find_url(string): \n",
        "    regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
        "    url = re.findall(regex,string)       \n",
        "    return [x[0] for x in url] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2H1mxpdaioGa"
      },
      "source": [
        "for i in zip(file_name_list, input_entity_list):\n",
        "  # print the file name\n",
        "  print(i)\n",
        "  # iterate over the phrases stored in entities.txt file\n",
        "  for j in i[1]:\n",
        "    # split them\n",
        "    split_version = j.split('\\t')\n",
        "\n",
        "    # Open the empty Research-problem.txt file in that folder\n",
        "    output_file = open(input_dir + i[0] + \"/\" + \"triples/\" + \"research-problem.txt\",\"a\")\n",
        "    \n",
        "    # If the sentence number of that phrase is between \"0 to 30\", then only process further \n",
        "    if(0<(int)(split_version[0])<=30):\n",
        "\n",
        "      # Now identify if that phrase is the \"only phrase\" extracted from that sentence \n",
        "      count_of_line = 0\n",
        "      for kk in i[1]:\n",
        "        if(kk.split('\\t')[0]==split_version[0]):\n",
        "          count_of_line += 1\n",
        "          if(count_of_line == 2):\n",
        "            break\n",
        "            \n",
        "      # \"Count_of_line\" stores the number of phrases extracted from that sentence.      \n",
        "      if(count_of_line==1):      \n",
        "        print(split_version)\n",
        "\n",
        "        # It can happen that the single extracted phrase might be a url, so we check it here \n",
        "        list_urls = find_url(split_version[3])\n",
        "\n",
        "        # Include it only if it is not a url \n",
        "        if (len(list_urls)==0): \n",
        "          output_file.write(\"(Contribution||has research problem||\" + split_version[3] + \")\" + \"\\n\")\n",
        "    output_file.close()  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}